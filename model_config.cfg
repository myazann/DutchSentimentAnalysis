[o3-mini]
repo_id = o3-mini
context_length = 200000
min_GPU_RAM = 0
provider = OPENAI

[GPT-4o]
repo_id = chatgpt-4o-latest
context_length = 128000
min_GPU_RAM = 0
provider = OPENAI

[GPT-4o-mini]
repo_id = gpt-4o-mini
context_length = 128000
min_GPU_RAM = 0
provider = OPENAI

[CLAUDE-3.5-SONNET]
repo_id = claude-3-5-sonnet-20240620
context_length = 200000
min_GPU_RAM = 0
provider = ANTHROPIC

[DEEPSEEK-V3]
repo_id = deepseek-chat
tokenizer = deepseek-ai/DeepSeek-V3
context_length = 64000
min_GPU_RAM = 0
provider = DEEPSEEK

[DEEPSEEK-R1]
repo_id = deepseek-reasoner
tokenizer = deepseek-ai/DeepSeek-R1
context_length = 64000
reason = True
min_GPU_RAM = 0
provider = DEEPSEEK

[LLAMA-3.3-70B-GGUF]
repo_id = bartowski/Llama-3.3-70B-Instruct-GGUF
file_name = Llama-3.3-70B-Instruct-Q4_K_M.gguf
tokenizer = meta-llama/Llama-3.3-70B-Instruct
context_length = 32000
min_GPU_RAM = 48

[LLAMA-3.3-70B]
repo_id = meta-llama/Llama-3.3-70B-Instruct
context_length = 128000
min_GPU_RAM = 160

[LLAMA-3.1-8B]
repo_id = meta-llama/Meta-Llama-3.1-8B-Instruct
context_length = 128000
min_GPU_RAM = 18

[NEMOTRON-LLAMA-3.1-70B]
repo_id = nvidia/Llama-3.1-Nemotron-70B-Instruct-HF
context_length = 128000
min_GPU_RAM = 160

[LLAMA-3.2-3B]
repo_id = meta-llama/Llama-3.2-3B-Instruct
context_length = 128000
min_GPU_RAM = 8

[LLAMA-3.2-1B]
repo_id = meta-llama/Llama-3.2-1B-Instruct
context_length = 128000
min_GPU_RAM = 3

[MINISTRAL-8B-INSTRUCT]
repo_id = mistralai/Ministral-8B-Instruct-2410
context_length = 32768
min_GPU_RAM = 20

[GEMMA-2-2B]
repo_id = google/gemma-2-2b-it
context_length = 8192
min_GPU_RAM = 12

[GEMMA-2-9B]
repo_id = google/gemma-2-9b-it
context_length = 8192
min_GPU_RAM = 24

[GEMMA-2-27B]
repo_id = google/gemma-2-27b-it
context_length = 8192
min_GPU_RAM = 80

[QWEN-2.5-7B-1M]
repo_id = Qwen/Qwen2.5-7B-Instruct-1M
context_length = 1000000
min_GPU_RAM = 30

[QWEN-2.5-14B-1M]
repo_id = Qwen/Qwen2.5-14B-Instruct-1M
context_length = 1000000
min_GPU_RAM = 45

[QWEN-2.5-14B-GGUF]
repo_id = bartowski/Qwen2.5-14B-Instruct-GGUF
tokenizer = Qwen/Qwen2.5-14B-Instruct
file_name = Qwen2.5-14B-Instruct-Q4_K_M.gguf
context_length = 32000
min_GPU_RAM = 12

[QWEN-2.5-72B-GGUF]
repo_id = bartowski/Qwen2.5-72B-Instruct-GGUF
file_name = Qwen2.5-72B-Instruct-Q4_K_M.gguf
tokenizer = Qwen/Qwen2.5-72B-Instruct
context_length = 32000
min_GPU_RAM = 60

[QWEN-2.5-VL-3B]
repo_id = Qwen/Qwen2.5-VL-3B-Instruct
context_length = 128000
min_GPU_RAM = 14

[QWEN-2.5-VL-7B]
repo_id = Qwen/Qwen2.5-VL-7B-Instruct
context_length = 128000
min_GPU_RAM = 30

[DEEPSEEK-R1-GGUF]
repo_id = unsloth/DeepSeek-R1-GGUF
file_name = DeepSeek-R1-UD-IQ1_M
tokenizer = deepseek-ai/DeepSeek-R1
context_length = 16000
min_GPU_RAM = 240

[DEEPSEEK-R1-DISTILL-QWEN-32B]
repo_id = deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
context_length = 128000
min_GPU_RAM = 100

[DEEPSEEK-R1-DISTILL-QWEN-32B-GGUF]
repo_id = unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF
file_name = DeepSeek-R1-Distill-Qwen-32B-Q4_K_M.gguf
tokenizer = deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
context_length = 128000
min_GPU_RAM = 36

[DEEPSEEK-R1-DISTILL-QWEN-7B]
repo_id = deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
context_length = 128000
min_GPU_RAM = 16

[DEEPSEEK-R1-DISTILL-LLAMA-8B]
repo_id = deepseek-ai/DeepSeek-R1-Distill-Llama-8B
context_length = 128000
min_GPU_RAM = 18

[DEEPSEEK-R1-DISTILL-LLAMA-8B-GGUF]
repo_id = unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF
file_name = DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf
tokenizer = meta-llama/Meta-Llama-3.1-8B-Instruct
context_length = 128000
min_GPU_RAM = 6

[GEITJE-7B-ULTRA]
repo_id = BramVanroy/GEITje-7B-ultra
context_length = 32000
min_GPU_RAM = 24

[ROBBERT-V2-EMOTION]
repo_id = DTAI-KULeuven/robbert-v2-dutch-sentiment
context_length = 512
min_GPU_RAM = 1

[FIETJE-2-CHAT]
repo_id = BramVanroy/fietje-2-chat
context_length = 2048
min_GPU_RAM = 6

[MULTILINGUAL-BERT]
repo_id = google-bert/bert-base-multilingual-cased
context_length = 512
min_GPU_RAM = 1

[XLM-ROBERTA-BASE]
repo_id = FacebookAI/xlm-roberta-base
context_length = 768
min_GPU_RAM = 3